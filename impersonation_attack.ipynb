{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dafaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Environment & Helpers Ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Device Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Reproducibility (Crucial for scientific results)\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)\n",
    "\n",
    "# 3. Physics-Aware Normalization Helpers\n",
    "# These allow us to add the pattern in \"Real World\" pixel space (0-1)\n",
    "# BEFORE the model normalizes it for processing.\n",
    "class ReIDNormalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(ReIDNormalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.Tensor(std).view(1, 3, 1, 1))\n",
    "    def forward(self, tensor): return (tensor - self.mean) / self.std\n",
    "\n",
    "class ReIDUnNormalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(ReIDUnNormalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.Tensor(std).view(1, 3, 1, 1))\n",
    "    def forward(self, tensor): return (tensor * self.std) + self.mean\n",
    "\n",
    "# Standard ImageNet Normalization\n",
    "norm_layer = ReIDNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).to(device)\n",
    "unnorm_layer = ReIDUnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).to(device)\n",
    "print(\"Environment & Helpers Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6812781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Public\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Uday Raj\\AppData\\Local\\Temp\\ipykernel_14268\\3006263140.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_b.load_state_dict(torch.load('model_b_market1501.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model B Loaded Successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_ROOT = './Market-1501-v15.09.15' # Ensure this matches your folder name\n",
    "\n",
    "# Standard Transforms\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256, 128), interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Data\n",
    "image_datasets = {\n",
    "    'gallery': datasets.ImageFolder(os.path.join(DATA_ROOT, 'bounding_box_test'), transform_test),\n",
    "    'query': datasets.ImageFolder(os.path.join(DATA_ROOT, 'query'), transform_test)\n",
    "}\n",
    "\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=False, num_workers=2) \n",
    "               for x in ['gallery', 'query']}\n",
    "\n",
    "# --- MODEL B (ResNet-50) ---\n",
    "class ModelB_ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=751):\n",
    "        super(ModelB_ResNet50, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(self.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        # Extract features (IDE baseline)\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "# Initialize and Load Weights\n",
    "model_b = ModelB_ResNet50(num_classes=751).to(device)\n",
    "if os.path.exists('model_b_market1501.pth'):\n",
    "    model_b.load_state_dict(torch.load('model_b_market1501.pth'))\n",
    "    print(\"Model B Loaded Successfully.\")\n",
    "else:\n",
    "    print(\"Error: 'model_b_market1501.pth' not found. Please copy it from your previous project.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd57892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS (UPDATED) ---\n",
    "\n",
    "def get_images_by_pid(dataset, target_pid, max_images=50):\n",
    "    found_images = []\n",
    "    for i in range(len(dataset)):\n",
    "        path, _ = dataset.imgs[i]\n",
    "        filename = os.path.basename(path)\n",
    "        pid = int(filename.split('_')[0])\n",
    "        if pid == target_pid:\n",
    "            img_tensor = dataset[i][0].unsqueeze(0)\n",
    "            found_images.append(img_tensor)\n",
    "            if len(found_images) >= max_images: break\n",
    "    if len(found_images) == 0: return None\n",
    "    return torch.cat(found_images, dim=0).to(device)\n",
    "\n",
    "def extract_features_robust(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model.get_embedding(imgs)\n",
    "            fnorm = torch.norm(outputs, p=2, dim=1, keepdim=True)\n",
    "            outputs = outputs.div(fnorm.expand_as(outputs))\n",
    "            features.append(outputs.cpu())\n",
    "    return torch.cat(features, dim=0)\n",
    "\n",
    "def get_id_and_cam(dataset):\n",
    "    pids = []\n",
    "    for path, _ in dataset.imgs:\n",
    "        filename = os.path.basename(path)\n",
    "        pids.append(int(filename.split('_')[0]))\n",
    "    return np.array(pids)\n",
    "\n",
    "def calculate_metrics_exact(q_feats, g_feats, g_pids, target_pid):\n",
    "    \"\"\"Calculates detailed metrics (R1, R5, mAP, SS) for a specific target ID.\"\"\"\n",
    "    # Compute Similarity\n",
    "    distmat = torch.mm(q_feats, g_feats.t())\n",
    "    distmat = distmat.cpu().numpy()\n",
    "    \n",
    "    cmc_scores = []\n",
    "    ap_scores = []\n",
    "    ss_scores = []\n",
    "    \n",
    "    for i in range(q_feats.size(0)):\n",
    "        sims = distmat[i]\n",
    "        indices = np.argsort(sims)[::-1]\n",
    "        \n",
    "        # Check Matches\n",
    "        matches = (g_pids[indices] == target_pid).astype(np.int32)\n",
    "        \n",
    "        # Similarity Score (SS) of top match\n",
    "        if matches.sum() > 0:\n",
    "            first_match_idx = np.where(matches == 1)[0][0]\n",
    "            ss_scores.append(sims[indices[first_match_idx]])\n",
    "        else:\n",
    "            ss_scores.append(0.0)\n",
    "            \n",
    "        if matches.sum() == 0: continue\n",
    "        \n",
    "        # CMC\n",
    "        cmc = matches.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "        cmc_scores.append(cmc[:10])\n",
    "        \n",
    "        # mAP\n",
    "        num_rel = matches.sum()\n",
    "        tmp_cmc = matches.cumsum()\n",
    "        tmp_cmc = [x / (j + 1.) for j, x in enumerate(tmp_cmc)]\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * matches\n",
    "        ap_scores.append(tmp_cmc.sum() / num_rel)\n",
    "\n",
    "    if len(cmc_scores) == 0: return {'rank-1':0,'rank-5':0,'mAP':0,'ss':0}\n",
    "\n",
    "    all_cmc = np.mean(cmc_scores, axis=0)\n",
    "    return {\n",
    "        'rank-1': all_cmc[0],\n",
    "        'rank-5': all_cmc[4],\n",
    "        'rank-10': all_cmc[9],\n",
    "        'mAP': np.mean(ap_scores),\n",
    "        'ss': np.mean(ss_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06c2a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impersonation Class Defined.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPERSONATION ATTACK CLASS ---\n",
    "\n",
    "class AdvPatternAttack_Impersonation:\n",
    "    def __init__(self, model, device, lr=0.05):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Jitter: Essential. If we don't jitter, the pattern overfits \n",
    "        # to the specific pixel alignment of the training images.\n",
    "        self.jitter = T.Compose([\n",
    "            T.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05))\n",
    "        ])\n",
    "\n",
    "    def generate_impersonation_pattern(self, adversary_imgs, target_imgs, num_steps=2500):\n",
    "        print(f\"--- Starting IMPERSONATION Attack ({num_steps} steps) ---\")\n",
    "        \n",
    "        # 1. Prepare Target Centroid (The \"Ghost\" we want to mimic)\n",
    "        with torch.no_grad():\n",
    "            # Un-normalize -> Re-normalize ensures we are in the model's feature space\n",
    "            clean_tgt = unnorm_layer(target_imgs)\n",
    "            clean_tgt.clamp_(0, 1)\n",
    "            target_input = norm_layer(clean_tgt)\n",
    "            \n",
    "            # Extract features of all target images\n",
    "            target_feats = self.model.get_embedding(target_input)\n",
    "            target_feats = torch.nn.functional.normalize(target_feats, p=2, dim=1)\n",
    "            \n",
    "            # Calculate Average (Centroid)\n",
    "            # This is more stable than picking just one target image\n",
    "            target_centroid = torch.mean(target_feats, dim=0, keepdim=True).detach()\n",
    "        \n",
    "        # 2. Prepare Adversary Images\n",
    "        with torch.no_grad():\n",
    "            clean_adv = unnorm_layer(adversary_imgs)\n",
    "            clean_adv.clamp_(0, 1)\n",
    "\n",
    "        # 3. Mask (Chest Only - Paper Exact)\n",
    "        mask = torch.zeros((1, 3, 256, 128)).to(self.device)\n",
    "        mask[:, :, 60:160, 20:108] = 1.0 \n",
    "        \n",
    "        # 4. Init Pattern\n",
    "        delta = torch.rand((1, 3, 256, 128)).to(self.device)\n",
    "        delta.requires_grad = True\n",
    "        \n",
    "        optimizer = optim.Adam([delta], lr=self.lr)\n",
    "        # Drop LR at step 1000 and 2000 to \"lock in\" the best features\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000, 2000], gamma=0.1)\n",
    "        \n",
    "        # Loss: 1.0 means \"Make them similar\", -1.0 means \"Make them different\"\n",
    "        cosine_loss = nn.CosineEmbeddingLoss(margin=0.0)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply Pattern\n",
    "            patch = delta * mask\n",
    "            jit_patch = self.jitter(patch)\n",
    "            \n",
    "            # Add to Adversary (Physics-Aware)\n",
    "            adv_raw = clean_adv + jit_patch\n",
    "            adv_raw.clamp_(0, 1) # Valid pixel range\n",
    "            \n",
    "            # Extract Features\n",
    "            adv_input = norm_layer(adv_raw)\n",
    "            adv_feats = self.model.get_embedding(adv_input)\n",
    "            \n",
    "            # --- LOSS FUNCTION ---\n",
    "            # We want Adversary Features to match Target Centroid\n",
    "            \n",
    "            # Create a label of 1s (Similar)\n",
    "            t_label = torch.ones(adv_feats.size(0)).to(self.device)\n",
    "            # Expand centroid to match batch size\n",
    "            t_centroid_batch = target_centroid.expand_as(adv_feats)\n",
    "            \n",
    "            # Calculate Loss & Scale Up\n",
    "            # Scaling by 20.0 prevents vanishing gradients when similarity is \"okay\" but not \"perfect\"\n",
    "            loss = cosine_loss(adv_feats, t_centroid_batch, t_label) * 20.0\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # PGD Clamp\n",
    "            with torch.no_grad():\n",
    "                delta.clamp_(0, 1)\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "        return delta.detach(), mask\n",
    "\n",
    "print(\"Impersonation Class Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fcffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Table 3 Replication (5 Pairs) ---\n",
      "\n",
      "[Pair 1] Adv: 791 -> Tgt: 1190\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 9.7505\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTE MULTI-PAIR IMPERSONATION (TABLE 3) ---\n",
    "\n",
    "def run_table3_replication(model, dataloaders, num_pairs=5, num_steps=2000):\n",
    "    print(f\"--- Starting Full Table 3 Replication ({num_pairs} Pairs) ---\")\n",
    "    \n",
    "    # 1. Get Valid PIDs\n",
    "    gal_pids = set([int(os.path.basename(path).split('_')[0]) for path, _ in dataloaders['gallery'].dataset.imgs])\n",
    "    qry_pids = set([int(os.path.basename(path).split('_')[0]) for path, _ in dataloaders['query'].dataset.imgs])\n",
    "    valid_pids = list(gal_pids.intersection(qry_pids))\n",
    "    valid_pids = [p for p in valid_pids if p > 0]\n",
    "    \n",
    "    # 2. Prepare Gallery Features (Static)\n",
    "    g_feats = extract_features_robust(model, dataloaders['gallery'], device)\n",
    "    g_pids = get_id_and_cam(dataloaders['gallery'].dataset)\n",
    "    # Move Gallery to GPU for fast calculation\n",
    "    g_feats = g_feats.to(device)\n",
    "    \n",
    "    # Storage\n",
    "    agg_tgt = {'rank-1': [], 'rank-5': [], 'rank-10': [], 'mAP': [], 'ss': []}\n",
    "    agg_slf = {'rank-1': [], 'rank-5': [], 'rank-10': [], 'mAP': [], 'ss': []}\n",
    "    \n",
    "    for i in range(num_pairs):\n",
    "        # Pick Pair\n",
    "        adv_pid, tgt_pid = random.sample(valid_pids, 2)\n",
    "        print(f\"\\n[Pair {i+1}] Adv: {adv_pid} -> Tgt: {tgt_pid}\")\n",
    "        \n",
    "        # Get Data\n",
    "        adv_gs = get_images_by_pid(dataloaders['gallery'].dataset, adv_pid)\n",
    "        tgt_gs = get_images_by_pid(dataloaders['gallery'].dataset, tgt_pid)\n",
    "        adv_ts = get_images_by_pid(dataloaders['query'].dataset, adv_pid)\n",
    "        \n",
    "        if adv_gs is None or tgt_gs is None: continue\n",
    "            \n",
    "        # Train Pattern\n",
    "        attacker = AdvPatternAttack_Impersonation(model, device, lr=0.05)\n",
    "        pat, mask = attacker.generate_impersonation_pattern(adv_gs, tgt_gs, num_steps=num_steps)\n",
    "        \n",
    "        # Generate Adversarial Queries\n",
    "        with torch.no_grad():\n",
    "            q_raw = unnorm_layer(adv_ts.to(device))\n",
    "            q_adv = norm_layer((q_raw + (pat * mask)).clamp(0, 1))\n",
    "            q_feat = torch.nn.functional.normalize(model.get_embedding(q_adv), p=2, dim=1)\n",
    "            \n",
    "        # Calc Metrics (Success: Match Target)\n",
    "        m_tgt = calculate_metrics_exact(q_feat, g_feats, g_pids, tgt_pid)\n",
    "        # Calc Metrics (Fail: Match Self)\n",
    "        m_slf = calculate_metrics_exact(q_feat, g_feats, g_pids, adv_pid)\n",
    "        \n",
    "        print(f\"   > Target Rank-1: {m_tgt['rank-1']:.1%}\")\n",
    "        \n",
    "        for k in agg_tgt:\n",
    "            agg_tgt[k].append(m_tgt[k])\n",
    "            agg_slf[k].append(m_slf[k])\n",
    "            \n",
    "    # Averages\n",
    "    avg_tgt = {k: np.mean(v) for k, v in agg_tgt.items()}\n",
    "    avg_slf = {k: np.mean(v) for k, v in agg_slf.items()}\n",
    "    return avg_tgt, avg_slf\n",
    "\n",
    "# Run it\n",
    "final_tgt, final_self = run_table3_replication(model_b, dataloaders, num_pairs=5, num_steps=2000)\n",
    "\n",
    "# --- PRINT TABLE 3 ---\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(f\"Table 3. Digital-environment attack results (Average of 5 Pairs)\")\n",
    "print(\"=\"*95)\n",
    "print(f\"{'Matched Person':<20} {'rank-1':<12} {'rank-5':<12} {'rank-10':<12} {'mAP':<12} {'ss':<12}\")\n",
    "print(\"-\" * 95)\n",
    "print(f\"{'Target (Success)':<20} {final_tgt['rank-1']:.1%}      {final_tgt['rank-5']:.1%}      {final_tgt['rank-10']:.1%}      {final_tgt['mAP']:.1%}      {final_tgt['ss']:.3f}\")\n",
    "print(f\"{'Adversary (Fail)':<20} {final_self['rank-1']:.1%}      {final_self['rank-5']:.1%}      {final_self['rank-10']:.1%}      {final_self['mAP']:.1%}      {final_self['ss']:.3f}\")\n",
    "print(\"=\"*95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5113a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_exact(q_feats, g_feats, g_pids, target_pid):\n",
    "    \"\"\"\n",
    "    Calculates Rank-1, Rank-5, Rank-10, mAP, and SS for a specific target ID.\n",
    "    \"\"\"\n",
    "    # Compute Cosine Similarity Matrix\n",
    "    distmat = torch.mm(q_feats, g_feats.t())\n",
    "    distmat = distmat.cpu().numpy()\n",
    "    \n",
    "    cmc_scores = []\n",
    "    ap_scores = []\n",
    "    ss_scores = [] \n",
    "    \n",
    "    for i in range(q_feats.size(0)):\n",
    "        # Get similarities for this query\n",
    "        sims = distmat[i]\n",
    "        # Sort descending (highest sim first)\n",
    "        indices = np.argsort(sims)[::-1]\n",
    "        \n",
    "        # 1. Find matches (Ground Truth)\n",
    "        matches = (g_pids[indices] == target_pid).astype(np.int32)\n",
    "        \n",
    "        # 2. Calculate SS (Similarity Score of the first correct match)\n",
    "        if matches.sum() > 0:\n",
    "            first_match_idx = np.where(matches == 1)[0][0]\n",
    "            # Score of the top-1 correct image\n",
    "            ss_scores.append(sims[indices[first_match_idx]])\n",
    "        else:\n",
    "            ss_scores.append(0.0)\n",
    "\n",
    "        # 3. Calculate CMC (Ranks)\n",
    "        if matches.sum() == 0: \n",
    "            continue\n",
    "            \n",
    "        cmc = matches.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "        cmc_scores.append(cmc[:10])\n",
    "        \n",
    "        # 4. Calculate mAP\n",
    "        num_rel = matches.sum()\n",
    "        tmp_cmc = matches.cumsum()\n",
    "        tmp_cmc = [x / (j + 1.) for j, x in enumerate(tmp_cmc)]\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * matches\n",
    "        ap_scores.append(tmp_cmc.sum() / num_rel)\n",
    "\n",
    "    # Averages\n",
    "    if len(cmc_scores) == 0:\n",
    "        return {'rank-1': 0.0, 'rank-5': 0.0, 'rank-10': 0.0, 'mAP': 0.0, 'ss': 0.0}\n",
    "        \n",
    "    all_cmc = np.mean(cmc_scores, axis=0)\n",
    "    mean_ap = np.mean(ap_scores)\n",
    "    mean_ss = np.mean(ss_scores)\n",
    "    \n",
    "    return {\n",
    "        'rank-1': all_cmc[0],\n",
    "        'rank-5': all_cmc[4],\n",
    "        'rank-10': all_cmc[9],\n",
    "        'mAP': mean_ap,\n",
    "        'ss': mean_ss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e7c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Table 3 Replication (5 Pairs) ---\n",
      "Test Pairs (Adv -> Tgt): [[1272, 624], [576, 33], [85, 219], [1131, 687], [1299, 870]]\n",
      "\n",
      "[Pair 1/5] Training: PID 1272 -> PID 624\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 14.1843\n",
      "Step 500, Loss: 7.8425\n",
      "Step 1000, Loss: 7.0179\n",
      "Step 1500, Loss: 7.6609\n",
      "   > Target Match R1: 20.0%\n",
      "   > Self Match R1:   0.0%\n",
      "\n",
      "[Pair 2/5] Training: PID 576 -> PID 33\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 14.0802\n",
      "Step 500, Loss: 7.0775\n",
      "Step 1000, Loss: 7.9431\n",
      "Step 1500, Loss: 7.9175\n",
      "   > Target Match R1: 0.0%\n",
      "   > Self Match R1:   0.0%\n",
      "\n",
      "[Pair 3/5] Training: PID 85 -> PID 219\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 13.1561\n",
      "Step 500, Loss: 7.9163\n",
      "Step 1000, Loss: 7.6638\n",
      "Step 1500, Loss: 7.9920\n",
      "   > Target Match R1: 0.0%\n",
      "   > Self Match R1:   50.0%\n",
      "\n",
      "[Pair 4/5] Training: PID 1131 -> PID 687\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 11.5202\n",
      "Step 500, Loss: 4.7454\n",
      "Step 1000, Loss: 5.5146\n",
      "Step 1500, Loss: 4.7513\n",
      "   > Target Match R1: 100.0%\n",
      "   > Self Match R1:   0.0%\n",
      "\n",
      "[Pair 5/5] Training: PID 1299 -> PID 870\n",
      "--- Starting IMPERSONATION Attack (2000 steps) ---\n",
      "Step 0, Loss: 10.4356\n",
      "Step 500, Loss: 5.4024\n",
      "Step 1000, Loss: 5.9370\n",
      "Step 1500, Loss: 5.2847\n",
      "   > Target Match R1: 0.0%\n",
      "   > Self Match R1:   16.7%\n",
      "\n",
      "===============================================================================================\n",
      "Table 3. Digital-environment attack results (Replicated on Model B - Avg of 5 Pairs)\n",
      "===============================================================================================\n",
      "Matched Person       rank-1       rank-5       rank-10      mAP          ss          \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Target (TS)          24.0%      34.0%      34.0%      12.5%      0.721\n",
      "Adversary (TS)       13.3%      26.7%      34.0%      13.1%      0.654\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "def run_table3_experiment(model, dataloaders, num_pairs=5, num_steps=2000):\n",
    "    print(f\"--- Starting Table 3 Replication ({num_pairs} Pairs) ---\")\n",
    "    \n",
    "    # 1. Get Valid PIDs\n",
    "    # We need PIDs that exist in both Gallery and Query\n",
    "    gal_pids = set([int(os.path.basename(path).split('_')[0]) for path, _ in dataloaders['gallery'].dataset.imgs])\n",
    "    qry_pids = set([int(os.path.basename(path).split('_')[0]) for path, _ in dataloaders['query'].dataset.imgs])\n",
    "    valid_pids = list(gal_pids.intersection(qry_pids))\n",
    "    valid_pids = [p for p in valid_pids if p > 0] # Remove junk\n",
    "    \n",
    "    # 2. Select Random Pairs\n",
    "    pairs = []\n",
    "    for _ in range(num_pairs):\n",
    "        pairs.append(random.sample(valid_pids, 2))\n",
    "        \n",
    "    print(f\"Test Pairs (Adv -> Tgt): {pairs}\")\n",
    "    \n",
    "    # Storage for averaging\n",
    "    agg_target_metrics = {'rank-1': [], 'rank-5': [], 'rank-10': [], 'mAP': [], 'ss': []}\n",
    "    agg_self_metrics =   {'rank-1': [], 'rank-5': [], 'rank-10': [], 'mAP': [], 'ss': []}\n",
    "    \n",
    "    # 3. Pre-compute Gallery Features (Static)\n",
    "    g_feats = extract_features_robust(model, dataloaders['gallery'], device)\n",
    "    g_pids = get_id_and_cam(dataloaders['gallery'].dataset)\n",
    "    g_feats = g_feats.to(device) # Move to GPU for speed\n",
    "    \n",
    "    # 4. Loop\n",
    "    for i, (adv_pid, tgt_pid) in enumerate(pairs):\n",
    "        print(f\"\\n[Pair {i+1}/{num_pairs}] Training: PID {adv_pid} -> PID {tgt_pid}\")\n",
    "        \n",
    "        # Get Data\n",
    "        adv_gs = get_images_by_pid(dataloaders['gallery'].dataset, adv_pid)\n",
    "        tgt_gs = get_images_by_pid(dataloaders['gallery'].dataset, tgt_pid)\n",
    "        adv_ts = get_images_by_pid(dataloaders['query'].dataset, adv_pid)\n",
    "        \n",
    "        if adv_gs is None or tgt_gs is None or adv_ts is None:\n",
    "            print(\"Skipping pair (data missing)\")\n",
    "            continue\n",
    "            \n",
    "        # Train Pattern\n",
    "        attacker = AdvPatternAttack_Impersonation(model, device, lr=0.05)\n",
    "        pat, mask = attacker.generate_impersonation_pattern(adv_gs, tgt_gs, num_steps=num_steps)\n",
    "        \n",
    "        # Generate Adversarial Query Features\n",
    "        with torch.no_grad():\n",
    "            q_raw = unnorm_layer(adv_ts.to(device))\n",
    "            q_adv = norm_layer((q_raw + (pat * mask)).clamp(0, 1))\n",
    "            q_feat_adv = torch.nn.functional.normalize(model.get_embedding(q_adv), p=2, dim=1)\n",
    "        \n",
    "        # --- METRIC A: TARGET MATCH (Success) ---\n",
    "        # \"Did we look like the Target?\" -> Ground Truth = tgt_pid\n",
    "        m_tgt = calculate_metrics_exact(q_feat_adv, g_feats, g_pids, tgt_pid)\n",
    "        \n",
    "        # --- METRIC B: ADVERSARY MATCH (Fail) ---\n",
    "        # \"Did we still look like ourselves?\" -> Ground Truth = adv_pid\n",
    "        m_self = calculate_metrics_exact(q_feat_adv, g_feats, g_pids, adv_pid)\n",
    "        \n",
    "        # Log & Store\n",
    "        print(f\"   > Target Match R1: {m_tgt['rank-1']:.1%}\")\n",
    "        print(f\"   > Self Match R1:   {m_self['rank-1']:.1%}\")\n",
    "        \n",
    "        for k in agg_target_metrics:\n",
    "            agg_target_metrics[k].append(m_tgt[k])\n",
    "            agg_self_metrics[k].append(m_self[k])\n",
    "\n",
    "    # 5. Compute Final Averages\n",
    "    avg_target = {k: np.mean(v) for k, v in agg_target_metrics.items()}\n",
    "    avg_self = {k: np.mean(v) for k, v in agg_self_metrics.items()}\n",
    "    \n",
    "    return avg_target, avg_self\n",
    "\n",
    "# --- EXECUTE & PRINT TABLE ---\n",
    "# Run on 5 pairs to get a robust average\n",
    "final_tgt, final_self = run_table3_experiment(model_b, dataloaders, num_pairs=5, num_steps=2000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(f\"Table 3. Digital-environment attack results (Replicated on Model B - Avg of 5 Pairs)\")\n",
    "print(\"=\"*95)\n",
    "print(f\"{'Matched Person':<20} {'rank-1':<12} {'rank-5':<12} {'rank-10':<12} {'mAP':<12} {'ss':<12}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Row 1: Matched as Target (Success)\n",
    "print(f\"{'Target (TS)':<20} {final_tgt['rank-1']:.1%}      {final_tgt['rank-5']:.1%}      {final_tgt['rank-10']:.1%}      {final_tgt['mAP']:.1%}      {final_tgt['ss']:.3f}\")\n",
    "\n",
    "# Row 2: Matched as Adversary (Failure)\n",
    "print(f\"{'Adversary (TS)':<20} {final_self['rank-1']:.1%}      {final_self['rank-5']:.1%}      {final_self['rank-10']:.1%}      {final_self['mAP']:.1%}      {final_self['ss']:.3f}\")\n",
    "print(\"=\"*95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286172a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30c9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e1fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d1f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78658ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6333f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba2d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222c528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6542243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17bbb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
